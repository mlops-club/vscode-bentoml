# architecture diagram: https://github.com/allegroai/clearml-server#system-design

version: "3.6"
services:

  apiserver:
    command:
      - apiserver
    container_name: clearml-apiserver
    image: allegroai/clearml:latest
    restart: unless-stopped
    volumes:
      - ./volumes/opt/clearml/logs:/var/log/clearml
      - ./volumes/opt/clearml/config:/opt/clearml/config
      - ./volumes/opt/clearml/data/fileserver:/mnt/fileserver
    depends_on:
      - redis
      - mongo
      - elasticsearch
      - fileserver
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_ELASTIC_SERVICE_PASSWORD: ${ELASTIC_PASSWORD}
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      CLEARML_SERVER_DEPLOYMENT_TYPE: ${CLEARML_SERVER_DEPLOYMENT_TYPE:-linux}
      CLEARML__apiserver__pre_populate__enabled: "true"
      CLEARML__apiserver__pre_populate__zip_files: "/opt/clearml/db-pre-populate"
      CLEARML__apiserver__pre_populate__artifacts_path: "/mnt/fileserver"
      CLEARML__services__async_urls_delete__enabled: "true"
      # these values come from here: https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server_security/#example-using-docker-compose
      # CLEARML__SECURE__HTTP__SESSION_SECRET__APISERVER: "NXC9QMHYXP95JPWEORC4"
      # CLEARML__SECURE__AUTH__TOKEN_SECRET: "hpwa08LCTCoTcempS4I141lI4KLejXCPb9i0opbSD6xSeK2UST"
      # CLEARML__SECURE__CREDENTIALS__APISERVER__USER_KEY: "NXC9QMHYXP95JPWEORC4"  # must be 30 chars
      # CLEARML__SECURE__CREDENTIALS__APISERVER__USER_SECRET: "hpwa08LCTCoTcempS4I141lI4KLejXCPb9i0opbSD6xSeK2UST"  # must be 50-60 chars
      # CLEARML__SECURE__CREDENTIALS__WEBSERVER__USER_KEY: "NXC9QMHYXP95JPWEORC4"
      # CLEARML__SECURE__CREDENTIALS__WEBSERVER__USER_SECRET: "hpwa08LCTCoTcempS4I141lI4KLejXCPb9i0opbSD6xSeK2UST"
      # CLEARML__SECURE__CREDENTIALS__TESTS__USER_KEY: "NXC9QMHYXP95JPWEORC4"
      # CLEARML__SECURE__CREDENTIALS__TESTS__USER_SECRET: "hpwa08LCTCoTcempS4I141lI4KLejXCPb9i0opbSD6xSeK2UST"
    ports:
      - "8008:8008"
    networks:
      - backend
      - frontend

  elasticsearch:
    networks:
      - backend
    container_name: clearml-elastic
    environment:
      ES_JAVA_OPTS: -Xms2g -Xmx2g -Dlog4j2.formatMsgNoLookups=true
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      bootstrap.memory_lock: "true"
      cluster.name: clearml
      cluster.routing.allocation.node_initial_primaries_recoveries: "500"
      cluster.routing.allocation.disk.watermark.low: 500mb
      cluster.routing.allocation.disk.watermark.high: 500mb
      cluster.routing.allocation.disk.watermark.flood_stage: 500mb
      discovery.zen.minimum_master_nodes: "1"
      discovery.type: "single-node"
      http.compression_level: "7"
      node.ingest: "true"
      node.name: clearml
      reindex.remote.whitelist: '*.*'
      xpack.monitoring.enabled: "false"
      xpack.security.enabled: "false"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2
    restart: unless-stopped
    volumes:
      - ./volumes/opt/clearml/data/elastic_7:/usr/share/elasticsearch/data
      - ./volumes/usr/share/elasticsearch/logs:/usr/share/elasticsearch/logs

  fileserver:
    networks:
      - backend
      - frontend
    command:
      - fileserver
    container_name: clearml-fileserver
    image: allegroai/clearml:latest
    environment:
      CLEARML__fileserver__delete__allow_batch: "true"
    restart: unless-stopped
    volumes:
      - ./volumes/opt/clearml/logs:/var/log/clearml
      - ./volumes/opt/clearml/data/fileserver:/mnt/fileserver
      - ./volumes/opt/clearml/config:/opt/clearml/config
    ports:
      - "8081:8081"

  mongo:
    networks:
      - backend
    container_name: clearml-mongo
    image: mongo:4.4.9
    restart: unless-stopped
    command: --setParameter internalQueryMaxBlockingSortMemoryUsageBytes=196100200
    volumes:
      - ./volumes/opt/clearml/data/mongo_4/db:/data/db
      - ./volumes/opt/clearml/data/mongo_4/configdb:/data/configdb

  redis:
    networks:
      - backend
    container_name: clearml-redis
    image: redis:5.0
    restart: unless-stopped
    volumes:
      - ./volumes/opt/clearml/data/redis:/data

  webserver:
    command:
      - webserver
    container_name: clearml-webserver
    # environment:
    #  CLEARML_SERVER_SUB_PATH : clearml-web # Allow Clearml to be served with a URL path prefix.
    image: allegroai/clearml:latest
    restart: unless-stopped
    depends_on:
      - apiserver
    ports:
      - "8080:80"
    networks:
      - backend
      - frontend
    # # added by eric:
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8080/api/v2.26/debug.ping"]
    #   interval: 1s
    #   timeout: 10s
    #   retries: 5
    #   start_period: 5s

  async_delete:
    depends_on:
      - apiserver
      - redis
      - mongo
      - elasticsearch
      - fileserver
    container_name: async_delete
    image: allegroai/clearml:latest
    networks:
      - backend
    restart: unless-stopped
    environment:
      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch
      CLEARML_ELASTIC_SERVICE_PORT: 9200
      CLEARML_ELASTIC_SERVICE_PASSWORD: ${ELASTIC_PASSWORD}
      CLEARML_MONGODB_SERVICE_HOST: mongo
      CLEARML_MONGODB_SERVICE_PORT: 27017
      CLEARML_REDIS_SERVICE_HOST: redis
      CLEARML_REDIS_SERVICE_PORT: 6379
      PYTHONPATH: /opt/clearml/apiserver
      CLEARML__services__async_urls_delete__fileserver__url_prefixes: "[${CLEARML_FILES_HOST:-}]"
    entrypoint:
      - python3
      - -m
      - jobs.async_urls_delete
      - --fileserver-host
      - http://fileserver:8081
    volumes:
      - ./volumes/opt/clearml/logs:/var/log/clearml

  agent-services-queue: &agent_defaults
    networks:
      - backend
    container_name: clearml-agent-services
    image: allegroai/clearml-agent-services:latest
    deploy:
      restart_policy:
        condition: on-failure
    privileged: true
    environment: &agent_defaults__environment
      CLEARML_HOST_IP: ${CLEARML_HOST_IP}
      CLEARML_WEB_HOST: ${CLEARML_WEB_HOST:-http://webserver:8080}
      CLEARML_API_HOST: http://apiserver:8008
      CLEARML_FILES_HOST: ${CLEARML_FILES_HOST:-http://fileserver:8081}
      CLEARML_API_ACCESS_KEY: ${CLEARML_API_ACCESS_KEY:-}
      CLEARML_API_SECRET_KEY: ${CLEARML_API_SECRET_KEY:-}
      CLEARML_AGENT_GIT_USER: ${CLEARML_AGENT_GIT_USER}
      CLEARML_AGENT_GIT_PASS: ${CLEARML_AGENT_GIT_PASS}
      CLEARML_AGENT_UPDATE_VERSION: ${CLEARML_AGENT_UPDATE_VERSION:->=0.17.0}
      CLEARML_AGENT_DEFAULT_BASE_DOCKER: "ubuntu:18.04"
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minioadmin}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION:-}
      AZURE_STORAGE_ACCOUNT: ${AZURE_STORAGE_ACCOUNT:-}
      AZURE_STORAGE_KEY: ${AZURE_STORAGE_KEY:-}
      GOOGLE_APPLICATION_CREDENTIALS: ${GOOGLE_APPLICATION_CREDENTIALS:-}
      CLEARML_WORKER_ID: "clearml-services"
      CLEARML_AGENT_DOCKER_HOST_MOUNT: "${PWD}/volumes/opt/clearml/agent:/root/.clearml"
      SHUTDOWN_IF_NO_ACCESS_KEY: 1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./volumes/opt/clearml/agent:/root/.clearml
    depends_on:
      - apiserver
    entrypoint: >
      bash -c "curl --retry 10 --retry-delay 10 --retry-connrefused 'http://apiserver:8008/debug.ping' && /usr/agent/entrypoint.sh"

  agent-sessions-queue:
    <<: *agent_defaults  # This is the alias
    container_name: clearml-agent-sessions
    environment:
      <<: *agent_defaults__environment
      CLEARML_AGENT_EXTRA_ARGS: "--queue sessions --create-queue --docker"
      CLEARML_WORKER_ID: "clearml-sessions"
    entrypoint: >
      bash -c "curl --retry 10 --retry-delay 10 --retry-connrefused 'http://apiserver:8008/debug.ping' && /usr/agent/entrypoint.sh --cpu-only"

  minio:
    image: minio/minio
    container_name: clearml-minio
    volumes:
      - ./volumes/opt/clearml/data/minio:/data
    environment:
      MINIO_ACCESS_KEY: "minioadmin"  # Replace with your actual access key
      MINIO_SECRET_KEY: "minioadmin"  # Replace with your actual secret key
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      - backend
      - frontend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

networks:
  backend:
    driver: bridge
  frontend:
    driver: bridge
  default:
    name: frontend